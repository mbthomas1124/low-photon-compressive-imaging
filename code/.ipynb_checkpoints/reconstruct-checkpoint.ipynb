{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23154,
     "status": "ok",
     "timestamp": 1688751108668,
     "user": {
      "displayName": "Matthew Thomas",
      "userId": "08635113081893331323"
     },
     "user_tz": 240
    },
    "id": "SF13oVHWcWKs",
    "outputId": "d18826fc-5a6c-4719-e274-493e367a77c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from piqa import MS_SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1688751109453,
     "user": {
      "displayName": "Matthew Thomas",
      "userId": "08635113081893331323"
     },
     "user_tz": 240
    },
    "id": "M6loVnlvwS9n",
    "outputId": "5944d84a-83a5-4f93-b82f-3fde4be06b71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# use CUDA processors if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch_directml.device()\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1688751564516,
     "user": {
      "displayName": "Matthew Thomas",
      "userId": "08635113081893331323"
     },
     "user_tz": 240
    },
    "id": "_U_G_CWP71Dw"
   },
   "outputs": [],
   "source": [
    "def get_img_path(digit, i):\n",
    "    # get the image path given the digit and the index of the image within the digit\n",
    "    # eg. get_img_path(4, 12) denotes the 13th image of digit 4\n",
    "    if digit == 0:\n",
    "        img_num = 9000\n",
    "    else:\n",
    "        img_num = (digit - 1) * 1000\n",
    "    img_num += (i+1)\n",
    "    return f'../data/labels/{digit}/image{img_num}.png'\n",
    "\n",
    "\n",
    "def gather_data(exp_type: str, detect_type: str, feature_name: str, num_features: int):\n",
    "    # prepare feature data and image paths\n",
    "    features = None\n",
    "    image_paths = []\n",
    "    digits = []\n",
    "    for digit in range(10):\n",
    "        fpath = f'../data/features/{exp_type}/{detect_type}/{feature_name}/{feature_name}{digit}.csv'\n",
    "        data = pd.read_csv(fpath, header=None)\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            dig = data[0][i]\n",
    "            image_paths.append(get_img_path(dig, i))\n",
    "            digits += [dig]\n",
    "\n",
    "        data = data.drop(0, axis=1)\n",
    "        data = data.iloc[:,:num_features]\n",
    "        if features is None:\n",
    "            features = data.to_numpy()\n",
    "        else:\n",
    "            features = np.append(features, data.to_numpy(), axis=0)\n",
    "\n",
    "    features = torch.tensor(features.astype(np.float32)).to(device)\n",
    "    return features, image_paths, digits\n",
    "\n",
    "# define data transformations\n",
    "def feature_transform(x: torch.Tensor):\n",
    "    # standardizes the features of a given data point\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    return x.sub(mean).div(std).to(device)\n",
    "\n",
    "\n",
    "def img_transform(y: str):\n",
    "    # transforms an image path to a usable tensor\n",
    "    image = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "    convert_tensor = A.Compose([A.ToFloat(max_value=255), ToTensorV2()])\n",
    "    try:\n",
    "        thingy = convert_tensor(image=image)['image']\n",
    "        return thingy.to(device)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(f\"cannot load image: {y}\")\n",
    "\n",
    "\n",
    "def digit_transform(y: int):\n",
    "    return y\n",
    "\n",
    "\n",
    "# create Datasets and DataLoaders\n",
    "class ReconstructionData(Dataset):\n",
    "    def __init__(self, raw_features: torch.Tensor, labels: list, feature_transform, label_transform: None):\n",
    "        self.features = raw_features\n",
    "        self.labels = labels\n",
    "        self.feature_transform = feature_transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.feature_transform(self.features[idx])\n",
    "        label = self.label_transform(self.labels[idx])\n",
    "        return data, label\n",
    "\n",
    "\n",
    "def prep_data(features, labels, val_size, feature_transform, label_transform, batch_size):\n",
    "    # create train and validation dataloaders\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=val_size)\n",
    "    train_data = ReconstructionData(train_features, train_labels, feature_transform, label_transform)\n",
    "    val_data = ReconstructionData(val_features, val_labels, feature_transform, label_transform)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "class ImgReconstructNN(nn.Module):\n",
    "    def __init__(self, num_features: int):\n",
    "        super(ImgReconstructNN, self).__init__()\n",
    "        self.reconstruct_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(1024, 4096),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(4096, (4*28)*(4*28)),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Unflatten(1, (1, 4*28, 4*28)),\n",
    "            nn.Conv2d(1,4,4,padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4,1,2,padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.reconstruct_stack(x)\n",
    "        return torch.nan_to_num(logits)\n",
    "\n",
    "\n",
    "class ImgReconstructClassifyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImgReconstructClassifyNN, self).__init__()\n",
    "        self.reconstruct_classify_stack = nn.Sequential(\n",
    "            nn.Linear(300, 2048),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(2048, (8*28)*(8*28)),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Unflatten(1, (1, 8*28, 8*28)),\n",
    "            nn.Conv2d(1, 1, 8, padding=4),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(1, 1, 4, padding=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(1, 1, 2, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(1, 10, 5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(10, 20, 5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(320, 50),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.reconstruct_classify_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_loop(dataloader: DataLoader, val_dataloader: DataLoader, model: nn.Module, loss_fn, optimizer, print_loss: bool, train_losses: list, val_losses: list) -> list:\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    train_losses.append(train_loss)\n",
    "    if print_loss:\n",
    "        print(f\"Avg batch loss: {train_loss:>8f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for (X, y) in val_dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "        train_loss /= num_batches\n",
    "        val_losses.append(test_loss)\n",
    "    model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def reconstruct_test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    print(\"Running Test Loop\")\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_psnr = 0\n",
    "    test_ssim = 0\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            print(f\"batch: {batch+1}\")\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_ssim += ssim(pred, y).item()\n",
    "            test_psnr += psnr(pred, y).item()\n",
    "\n",
    "            # display 1 original and predicted image from each batch\n",
    "            transform = transforms.ToPILImage()\n",
    "            orig_data = y[0]\n",
    "            orig_img = transform(orig_data)\n",
    "            pred_data = pred[0]\n",
    "            pred_img = transform(pred_data)\n",
    "            diff_data = torch.abs(pred_data - orig_data)\n",
    "            diff_img = transform(diff_data)\n",
    "            print(\"original:\")\n",
    "            display(orig_img)\n",
    "            print(\"predicted:\")\n",
    "            display(pred_img)\n",
    "            print(\"difference:\")\n",
    "            display(diff_img)\n",
    "            print()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_psnr /= num_batches\n",
    "    test_ssim /= num_batches\n",
    "    print(f\"Avg batch loss: {test_loss:>8f}\")\n",
    "    print(f\"Avg batch PSNR: {test_psnr:>8f}\")\n",
    "    print(f\"Avg batch SSIM: {test_ssim:>8f}\")\n",
    "    return model, test_loss, test_psnr, test_ssim\n",
    "\n",
    "\n",
    "def reconstruct_test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    print(\"Running Test Loop\")\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_psnr = 0\n",
    "    test_ssim = 0\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            print(f\"batch: {batch+1}\")\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_ssim += ssim(pred, y).item()\n",
    "            test_psnr += psnr(pred, y).item()\n",
    "\n",
    "            # display 1 original and predicted image from each batch\n",
    "            transform = transforms.ToPILImage()\n",
    "            orig_data = y[0]\n",
    "            orig_img = transform(orig_data)\n",
    "            pred_data = pred[0]\n",
    "            pred_img = transform(pred_data)\n",
    "            diff_data = torch.abs(pred_data - orig_data)\n",
    "            diff_img = transform(diff_data)\n",
    "            print(\"original:\")\n",
    "            display(orig_img)\n",
    "            print(\"predicted:\")\n",
    "            display(pred_img)\n",
    "            print(\"difference:\")\n",
    "            display(diff_img)\n",
    "            print()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_psnr /= num_batches\n",
    "    test_ssim /= num_batches\n",
    "    print(f\"Avg batch loss: {test_loss:>8f}\")\n",
    "    print(f\"Avg batch PSNR: {test_psnr:>8f}\")\n",
    "    print(f\"Avg batch SSIM: {test_ssim:>8f}\")\n",
    "    return model, test_loss, test_psnr, test_ssim\n",
    "\n",
    "\n",
    "def reconstruct_test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    print(\"Running Test Loop\")\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_psnr = 0\n",
    "    test_ssim = 0\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            print(f\"batch: {batch+1}\")\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_ssim += ssim(pred, y).item()\n",
    "            test_psnr += psnr(pred, y).item()\n",
    "\n",
    "            # display 1 original and predicted image from each batch\n",
    "            transform = transforms.ToPILImage()\n",
    "            orig_data = y[0]\n",
    "            orig_img = transform(orig_data)\n",
    "            pred_data = pred[0]\n",
    "            pred_img = transform(pred_data)\n",
    "            diff_data = torch.abs(pred_data - orig_data)\n",
    "            diff_img = transform(diff_data)\n",
    "            print(\"original:\")\n",
    "            display(orig_img)\n",
    "            print(\"predicted:\")\n",
    "            display(pred_img)\n",
    "            print(\"difference:\")\n",
    "            display(diff_img)\n",
    "            print()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_psnr /= num_batches\n",
    "    test_ssim /= num_batches\n",
    "    print(f\"Avg batch loss: {test_loss:>8f}\")\n",
    "    print(f\"Avg batch PSNR: {test_psnr:>8f}\")\n",
    "    print(f\"Avg batch SSIM: {test_ssim:>8f}\")\n",
    "    return model, test_loss, test_psnr, test_ssim\n",
    "\n",
    "\n",
    "def train_model(train_dataloader, test_dataloader, model, loss_fn, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for t in range(epochs):\n",
    "        print_loss = False\n",
    "        if (t % 8 == 7) or (t==0):\n",
    "            print_loss = True\n",
    "            print(\"-------------------------------\")\n",
    "            print(f\"Epoch {t+1}\")\n",
    "        train_losses, val_losses = train_loop(train_dataloader, test_dataloader, model, loss_fn, optimizer, print_loss, train_losses, val_losses)\n",
    "    print(\"Done!\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses: list, val: bool) -> None:\n",
    "    losses = np.array(torch.tensor(losses).cpu())\n",
    "    plt.plot(losses, color='red')\n",
    "    plt.ylabel('Average Batch Loss') #set the label for y axis\n",
    "    plt.xlabel('Epoch') #set the label for x-axis\n",
    "    if val:\n",
    "        plt.title(\"Validation Loss over Epochs\") #set the title of the graph\n",
    "    else:\n",
    "        plt.title(\"Training Loss over Epochs\") #set the title of the graph\n",
    "    plt.show() #display the graph\n",
    "\n",
    "\n",
    "l1 = nn.L1Loss()\n",
    "class L1_SSIM_loss(MS_SSIM):\n",
    "    def forward(self, x, y):\n",
    "        return (0.5 * (1. - super().forward(x, y))) + (0.5 * l1.forward(x, y))\n",
    "\n",
    "\n",
    "def reconstruct(exp_type, detect_type, feature_name, batch_size, val_size, epochs, loss_fn, learning_rate, num_features):\n",
    "    print(f\"\\nTRAINING RECONSRTUCTION MODEL FOR: {feature_name}\\n\")\n",
    "    print(\"PART 1\")\n",
    "    features, image_paths, digits = gather_data(exp_type, detect_type, feature_name, num_features)\n",
    "    print(\"PART 2\")\n",
    "    train_dataloader, val_dataloader = prep_data(features, image_paths, val_size, feature_transform, img_transform, batch_size)\n",
    "    print(\"PART 3\")\n",
    "    model = ImgReconstructNN(num_features).to(device)\n",
    "    print(model)\n",
    "    print(\"PART 4\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    print(\"PART 5\")\n",
    "    train_losses, val_losses = train_model(train_dataloader, val_dataloader, model, loss_fn, optimizer, epochs)\n",
    "    print(\"PART 6\")\n",
    "    plot_loss(train_losses, False)\n",
    "    plot_loss(val_losses, True)\n",
    "    print(\"PART 7\")\n",
    "    model, val_loss, val_psnr, val_ssim = reconstruct_test_loop(val_dataloader, model, loss_fn)\n",
    "    saved_losses_loc = f'../data/features/{exp_type}/{detect_type}/{feature_name}/reconstruct_losses.pt'\n",
    "    torch.save((train_losses, val_loss), saved_losses_loc)\n",
    "    return val_loss, val_psnr, val_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9288588,
     "status": "error",
     "timestamp": 1688763292659,
     "user": {
      "displayName": "Matthew Thomas",
      "userId": "08635113081893331323"
     },
     "user_tz": 240
    },
    "id": "IxTdWIe8-PaV",
    "outputId": "a301adeb-3bd9-4697-c356-c8d0794faad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING RECONSRTUCTION MODEL FOR: sNdn40by40and240m\n",
      "\n",
      "PART 1\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "(100, 300)\n",
      "PART 2\n",
      "PART 3\n",
      "ImgReconstructNN(\n",
      "  (reconstruct_stack): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=1024, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "    (4): Linear(in_features=4096, out_features=12544, bias=True)\n",
      "    (5): Sigmoid()\n",
      "    (6): Unflatten(dim=1, unflattened_size=(1, 112, 112))\n",
      "    (7): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(4, 1, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Sigmoid()\n",
      "  )\n",
      ")\n",
      "PART 4\n",
      "PART 5\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Avg batch loss: 0.524150\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "batch_size = 50\n",
    "val_size = 0.2\n",
    "epochs = 128\n",
    "learning_rate = 1e-3\n",
    "loss_fn = L1_SSIM_loss(window_size=2, n_channels=1).to(device)\n",
    "\n",
    "num_features = 300\n",
    "\n",
    "###############################################################################\n",
    "# IMAGE RECONSTRUCTION\n",
    "###############################################################################\n",
    "\n",
    "# VARIED NOISE LEVELS\n",
    "results = {}\n",
    "results['detection_type'] = []\n",
    "results['filename'] = []\n",
    "results['val_loss'] = []\n",
    "results['val_psnr'] = []\n",
    "results['val_ssim'] = []\n",
    "for filename in os.listdir(f'../data/features/withASE/direct'):\n",
    "    val_loss, val_psnr, val_ssim = reconstruct(\"withASE\", \"direct\", filename, batch_size, val_size, epochs, loss_fn, learning_rate, num_features)\n",
    "    results['detection_type'].append(\"Direct Detection\")\n",
    "    results['filename'].append(filename)\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['val_psnr'].append(val_psnr)\n",
    "    results['val_ssim'].append(val_ssim)\n",
    "for filename in os.listdir(f'../data/features/withASE/QPMS'):\n",
    "    val_loss, val_psnr, val_ssim = reconstruct(\"withASE\", \"QPMS\", filename, batch_size, val_size, epochs, loss_fn, learning_rate, num_features)\n",
    "    results['detection_type'].append(\"QPMS\")\n",
    "    results['filename'].append(filename)\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['val_psnr'].append(val_psnr)\n",
    "    results['val_ssim'].append(val_ssim)\n",
    "results = pd.DataFrame.from_dict(results)\n",
    "filename = f'../data/features/withASE/{num_features}_reconstruction_results.csv'\n",
    "results.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "# VARIED INTEGRATION TIMES\n",
    "results = {}\n",
    "results['detection_type'] = []\n",
    "results['filename'] = []\n",
    "results['val_loss'] = []\n",
    "results['val_psnr'] = []\n",
    "results['val_ssim'] = []\n",
    "for filename in os.listdir(f'../data/features/withoutASE/direct'):\n",
    "    val_loss, val_psnr, val_ssim = reconstruct(\"withoutASE\", \"direct\", filename, batch_size, val_size, epochs, loss_fn, learning_rate, num_features)\n",
    "    results['detection_type'].append(\"Direct Detection\")\n",
    "    results['filename'].append(filename)\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['val_psnr'].append(val_psnr)\n",
    "    results['val_ssim'].append(val_ssim)\n",
    "for filename in os.listdir(f'../data/features/withoutASE/QPMS'):\n",
    "    val_loss, val_psnr, val_ssim = reconstruct(\"withoutASE\", \"QPMS\", filename, batch_size, val_size, epochs, loss_fn, learning_rate, num_features)\n",
    "    results['detection_type'].append(\"QPMS\")\n",
    "    results['filename'].append(filename)\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['val_psnr'].append(val_psnr)\n",
    "    results['val_ssim'].append(val_ssim)\n",
    "results = pd.DataFrame.from_dict(results)\n",
    "filename = f'../data/features/withoutASE/reconstruction_results.csv'\n",
    "results.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
